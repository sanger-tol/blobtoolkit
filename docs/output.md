# sanger-tol/blobtoolkit: Output

## Introduction

This document describes the output produced by the pipeline.

The directories listed below will be created in the results directory after the pipeline has finished. All paths are relative to the top-level results directory.

The directories comply with Tree of Life's canonical directory structure.

## Pipeline overview

The pipeline is built using [Nextflow](https://www.nextflow.io/) and processes data using the following steps:

- [BlobDir](#blobdir) - Output files viewable on a [BlobToolKit viewer](https://github.com/blobtoolkit/blobtoolkit)
- [Static plots](#static-plots) - Static versions of the BlobToolKit plots
- [BUSCO](#busco) - BUSCO results
- [Repeat masking](#repeat-masking) - Masked repeats (optional)
- [Read alignments](#read-alignments) - Aligned reads (optional)
- [Read coverage](#read-coverage) - Read coverage tracks
- [Base content](#base-content) - _k_-mer statistics (for k &le; 4)
- [MultiQC](#multiqc) - Aggregate report describing results from the whole pipeline
- [Pipeline information](#pipeline-information) - Report metrics generated during the workflow execution

### BlobDir

The files in the BlobDir dataset which is used to create the online interactive assessments.

<details markdown="1">
<summary>Output files</summary>

- `blobtoolkit/`
  - `<assembly-name>/`
    - `*.json.gz`: files generated from genome and alignment coverage statistics.

More information about visualising the data in the [BlobToolKit repository](https://github.com/blobtoolkit/blobtoolkit/tree/main/src/viewer)

</details>

### Static plots

Images generated from the above blobdir using the [blobtk](https://github.com/blobtoolkit/blobtk) tool.

<details markdown="1">
<summary>Output files</summary>

- `blobtoolkit/`
  - `plots/`
    - `*.png` or `*.svg`, depending on the selected output format: static versions of the BlobToolKit plots.

</details>

### BUSCO

BUSCO results generated by the pipeline (all BUSCO lineages that match the claassification of the species).

<details markdown="1">
<summary>Output files</summary>

- `busco/`
  - `<lineage-name>/`
    - `short_summary.json`: BUSCO scores for that lineage as a tab-separated file.
    - `short_summary.tsv`: BUSCO scores for that lineage as JSON.
    - `short_summary.txt`: BUSCO scores for that lineage as formatted text.
    - `full_table.tsv`: Coordinates of the annotated BUSCO genes as a tab-separated file.
    - `missing_busco_list.tsv`: List of the BUSCO genes that could not be found.
    - `*_busco_sequences.tar.gz`: Sequences of the annotated BUSCO genes. 1 _tar_ archive for each of the three annotation levels (`single_copy`, `multi_copy`, `fragmented`), with 1 file per gene.
    - `hmmer_output.tar.gz`: Archive of the HMMER alignment scores.

</details>

### Repeat masking

Reults from the repeat-masker step -- only if the pipeline is run with `--mask`.

<details markdown="1">
<summary>Output files</summary>

- `repeats/`
  - `windowmasker/`
    - `<accession>.fasta`: masked assembly in Fasta format.
    - `<accession>.obinary`: frequency counts of repeats, in windowmasker's own binary format.

</details>

### Read alignments

Read alignments in BAM format -- only if the pipeline is run with `--align`.

<details markdown="1">
<summary>Output files</summary>

- `read_mapping/`
  - `<datatype>/`
    - `<sample>.bam`: alignments of that sample's reads in BAM format.

</details>

### Read coverage

Read coverage statistics as computed by the pipeline.
Those files are the raw data used to build the BlobDir.

<details markdown="1">
<summary>Output files</summary>

- `read_mapping/`
  - `<datatype>/`
    - `<sample>.coverage.1k.bed.gz`: Bedgraph file with the coverage of the alignments of that sample per 1 kbp windows.

</details>

### Base content

_k_-mer statistics.
Those files are the raw data used to build the BlobDir.

<details markdown="1">
<summary>Output files</summary>

- `base_content/`
  - `<assembly-name>_*nuc_windows.tsv.gz`: Tab-separated files with the counts of every _k_-mer for k &le; 4 in 1 kbp windows. The first three columns correspond to the coordinates (sequence name, start, end), followed by each _k_-mer.
  - `<assembly-name>_freq_windows.tsv.gz`: Tab-separated files with frequencies derived from the _k_-mer counts.

</details>

### MultiQC

<details markdown="1">
<summary>Output files</summary>

- `multiqc/`
  - `multiqc_report.html`: a standalone HTML file that can be viewed in your web browser.
  - `multiqc_data/`: directory containing parsed statistics from the different tools used in the pipeline.
  - `multiqc_plots/`: directory containing static images from the report in various formats.

</details>

[MultiQC](http://multiqc.info) is a visualization tool that generates a single HTML report summarising all samples in your project. Some of the pipeline QC results are visualised in the report and further statistics are available in the report data directory.

Results generated by MultiQC collate pipeline QC from supported tools. The pipeline has special steps which also allow the software versions to be reported in the MultiQC output for future traceability. For more information about how to use MultiQC reports, see <http://multiqc.info>.

### Pipeline information

<details markdown="1">
<summary>Output files</summary>

- `pipeline_info/blobtoolkit/`
  - Reports generated by Nextflow: `execution_report.html`, `execution_timeline.html`, `execution_trace.txt` and `pipeline_dag.dot`/`pipeline_dag.svg`.
  - Reports generated by the pipeline: `pipeline_report.html`, `pipeline_report.txt` and `software_versions.yml`. The `pipeline_report*` files will only be present if the `--email` / `--email_on_fail` parameter's are used when running the pipeline.
  - Reformatted samplesheet files used as input to the pipeline: `samplesheet.valid.csv`.
  - Parameters used by the pipeline run: `params.json`.

</details>

[Nextflow](https://www.nextflow.io/docs/latest/tracing.html) provides excellent functionality for generating various reports relevant to the running and execution of the pipeline. This will allow you to troubleshoot errors with the running of the pipeline, and also provide you with other information such as launch commands, run times and resource usage.
